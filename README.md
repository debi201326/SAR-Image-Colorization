**Cross-Domain SAR to RGB Image Translation using Conditional GAN with Perceptual Guidance **

This project presents a deep learning-based approach for colorizing Synthetic Aperture Radar (SAR) images using a Conditional Generative Adversarial Network (cGAN). SAR images, though effective in capturing high-resolution surface data under all weather and lighting conditions, are grayscale and lack the semantic depth of optical images, making human interpretation challenging. To bridge this gap, we employ a U-Net-based generator to convert single-channel SAR inputs into colorized RGB outputs, guided by a discriminator that enforces texture realism. The model is trained using a composite loss function combining perceptual loss (from VGG19), pixel-wise L1 loss, and adversarial loss to enhance structural accuracy and visual fidelity. Trained on a paired SAR-optical dataset covering diverse land cover classes, the method generates colorized images that preserve spatial detail and exhibit high visual realism, making it suitable for practical remote sensing applications.
